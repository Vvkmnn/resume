%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode
% Awesome CV LaTeX Template for Cover Letter
%
% This template has been downloaded from:
% https://github.com/posquit0/Awesome-CV
%
% Authors:
% Claud D. Park <posquit0.bj@gmail.com>
% Lars Richter <mail@ayeks.de>
%
% Template license:
% CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0/)
%


%-------------------------------------------------------------------------------
% CONFIGURATIONS
%-------------------------------------------------------------------------------
% A4 paper size by default, use 'letterpaper' for US letter
\documentclass[11pt, a4paper]{awesome-cv}

% Configure page margins with geometry
\geometry{left=1.4cm, top=.8cm, right=1.4cm, bottom=1.8cm, footskip=.5cm}

% Specify the location of the included fonts
\fontdir[fonts/]

% Color for highlights
% Awesome Colors: awesome-emerald, awesome-skyblue, awesome-red, awesome-pink, awesome-orange
%                 awesome-nephritis, awesome-concrete, awesome-darknight
\colorlet{awesome}{awesome-red}
% Uncomment if you would like to specify your own color
% \definecolor{awesome}{HTML}{CA63A8}

% Colors for text
% Uncomment if you would like to specify your own color
% \definecolor{darktext}{HTML}{414141}
% \definecolor{text}{HTML}{333333}
% \definecolor{graytext}{HTML}{5D5D5D}
% \definecolor{lighttext}{HTML}{999999}

% Set false if you don't want to highlight section with awesome color
\setbool{acvSectionColorHighlight}{false}

% If you would like to change the social information separator from a pipe (|) to something else
\renewcommand{\acvHeaderSocialSep}{\quad\textbar\quad}


%-------------------------------------------------------------------------------
%	PERSONAL INFORMATION
%	Comment any of the lines below if they are not required
%-------------------------------------------------------------------------------
% Available options: circle|rectangle,edge/noedge,left/right
\photo[circle,edge,left]{./img/vLogoWhite.png}
\name{Vivek}{Menon}
\position{Data Scientist{\enskip\cdotp\enskip}AI Researcher{\enskip\cdotp\enskip}Technical Lead}
\address{5 St. Joseph Street, Toronto, Ontario, M4Y 0B6,  Canada}

\homepage{vvkmnn.xyz}
\mobile{(+1) 416-846-0630}
\email{vvkmnn@gmail.com}
\linkedin{vvkmnn}
\github{vvkmnn}
% \gitlab{gitlab-id}
% \stackoverflow{SO-id}{SO-name}
% \twitter{@vvkmnn}
% \skype{skype-id}
% \reddit{vvkmnn}
% \medium{vvkmnn}
% \googlescholar{google scholar-id}{name-to-display}
%% \firstname and \lastname will be used
% \googlescholar{googlescholar-id}{}
% \extrainfo{extra informations}

% \quote{``Be the change that you want to see in the world."}




% %-------------------------------------------------------------------------------
% %	LETTER INFORMATION
% %	All of the below lines must be filled out
% %-------------------------------------------------------------------------------
% The company being applied to
\recipient
  {Google Residency 2020 Application}
  {}
% The date on the letter, default is the date of compilation
\letterdate{\today}
% The title of the letter
\lettertitle{}
% How the letter is opened
\letteropening{}
% How the letter is closed
\letterclosing{}
% Any enclosures with the letter
\letterenclosure[Attached]{CV, Transcript, Certificates}
%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\RequirePackage{amsmath}
\documentclass[letterpaper]{awesome-cv}
\geometry{left=2cm, top=1.5cm, right=2cm, bottom=2cm, footskip=.5cm}

%% Math Font setup
\defaultfontfeatures{Scale=MatchLowercase}
\unimathsetup{partial=upright} % Source Sans Pro contains \partial at U+2202.
% If you should actually need \mitpartial, \mbfpartial, etc., load them from a
% math font.  If you want an italic \partial, load from SourceSansPro-It.
\setmathfont{STIX2Math.otf}
% Sets math symbols present in Source Sans Pro and that work. You still might
% not, e.g. want ≥ and ≤ from the text font if ≰ does not match them.
\setmathfont[range={up,"21,"23-"25,"2A-"2F,"5C,"5E-"5F,"7E,
                      `¬,`±,`·,`×,`÷,`≤,`≥,`≠,`≈,`∞,`√}
              ]{SourceSansPro-Regular.otf}
% Must load separately, or this will prevent autodetection of the upright math
% alphabet:
\setmathfont[range=\partial]{SourceSansPro-Regular.otf}
\setmathfont[range={it}]{SourceSansPro-RegularIt.otf}
\setmathfont[range={bfup}]{SourceSansPro-Bold.otf}
\setmathfont[range={bfit}]{SourceSansPro-BoldIt.otf}
% Greek variant letters Missing from Source Sans Pro:
\setmathfont[range={\mupalpha, \mupdelta, \mupbeta, \mupsigma, \mupmu, \mupvarepsilon,\mupvarphi,\mupvartheta,\mupvarpi,
                      \mupvarkappa,\mupvarrho,\mupvarTheta,\nabla,
                      \mitvarepsilon,\mitvarphi,\mitvartheta,\mitvarpi,
                      \mitvarkappa,\mitvarrho,\mitvarTheta,\mitnabla,
                      \mbfvarepsilon,\mbfvarphi,\mbfvartheta,\mbfvarpi,
                      \mbfvarkappa,\mbfvarrho,\mbfvarTheta,\mbfnabla,
                      \mbfitvarepsilon,\mbfitvarphi,\mbfitvartheta,
                      \mbfitvarpi,\mbfitvarkappa,\mbfitvarrho,
                      \mbfitvarTheta,\mbfitnabla }
               ]{GFSNeohellenicMath.otf}
% By default, \mathscr is the same as \mathcal, but several math fonts provde
% two separate alphabets as variants.
\setmathfont[range={scr,bfscr},
               Scale=MatchUppercase,
               StylisticSet=1
              ]{STIX2Math.otf}

% Workaround for \operatorname and \mathrm:
\setmathrm{SourceSansPro}[
  UprightFont = *-Regular ,
  BoldFont = *-Bold ,
  Ligatures = {Common, TeX },
  Extension = .otf ]
\setoperatorfont\mathup

% math commands

% new commands
\renewcommand{\C}{\mathbb C}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}

% new operators
\DeclareMathOperator{\Gal}{Gal}

%----------------------------------------------------------------------------------------
%   CODE
%----------------------------------------------------------------------------------------

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

%-------------------------------------------------------------------------------
\begin{document}

% Print the header with above personal informations
% Give optional argument to change alignment(C: center, L: left, R: right)
\makecvheader[R]

% Print the footer with 3 arguments(<left>, <center>, <right>)
% Leave any of these blank if they are not needed
\makecvfooter
{\thepage} % {\today}
{\href{https://vvkmnn.xyz/}{Vivek Menon}~~~·~~~Cover Letter}
{\today}% {\thepage}


% Print the title with above letter informations
% \makelettertitle

%-------------------------------------------------------------------------------
%	LETTER CONTENT
%-------------------------------------------------------------------------------
\begin{cvletter}

  \lettersection{Personal Statement}

I am applying to this Masters Program in Machine Learning and Data
Science because my primary research interest is in solving complex problems
using the simplest AI possible.

I believe learning cutting edge techniques while pursuing my career will allow me
to apply these algorithms to new domains quickly. Much academic effort today seems directed towards the development of AI with the assumption of optimal computing environments, volumes of clean data, or hybrid future computing paradigms like quantum variational algorithms. While
there can no doubt such technologies provide an invaluable opportunity to cheat
Moore's Law or access some NP-hard problems, it seems increasingly difficult to
see how anyone but large players with excess resources could afford to
experiment with such novel technologies in live settings.

Over the past 5 years, I have built a variety of Data Science solutions, pipelines, and proofs-of-concept for companies like Pfizer, TD, Honda, \& Apple to generally tackle the problems in the following domains:

\begin{itemize}
\item Predictive Consumer Behavior (Clickstreams, Analytics, Conversion, \ldots{})
\item Quantitative Financial Investing (Reinforcement, Multi-agent, Bayesian, \ldots{})
\item Strategic Consulting (Behavioural Clustering, KPI Regression, Simulation, \ldots{})
\end{itemize}

In that time, one thing I have found consistent common across these diverse
verticals is confusion. Whereas tools like Keras and Tensorflow have made it
possible to trade modular components to intelligent systems and accelerate the
field academically; most private sectors, despite the increasing optimism,
continue to discover painfully that having high-quality AI depends more on
having a high-quality data environment, sanitizing information, building tools,
and testing many models, before one can even begin to do something
intelligent with AI. There is a gap between the people who
research AI, and the people who need it applied, and I believe I am well suited
to help bridge that gap.

I cannot know what the future holds, but now that AI has been discovered, it seems prudent to iterate and optimize until we discover the simplest variants that we can use in business
applications.  Academically, we will always need to research and develop
multiple approaches, but I have discovered that in the field is that the primary
KPI for most technologies is not novelty or beauty, but rather ROI. I believe an
optimal practitioner must not only be aware of the latest research, but also
of where it might have the most immediate impact.

There can be no doubt that this new class of AI has achieved new levels of
excellence, beginning to call into question what it even means to be an expert
anymore; however, now that it has been discovered it, what of the myriad of ways
to improve it? Ways to apply Monte Carlo Tree Search to sparse data, encourage
reinforcement strategies, or scale self-play via minimal dependencies? As a
purveyor of multiple Cloud ML APIs, I have found many to be excellent
alternatives to local training in many domains, and believe that perhaps one day
soon one might even call a variant of an AlphaZero API against some arbitrary
game. When that day comes, it will become paramount to understand not the just
the 'what' behind the capabilities of AI, but the 'why'; which is why I seek to
better myself through research.

Such work may not be glamorous, but I believe it is that practicality that may
be pivotal in helping transition AIs from narrow to general. By taking these
algorithms from research to practice, I believe we may discover a tool that
may level the playing field in the face of inequity, disease, and/or climate change.


% \lettersection{What are your primary research interests and why are they important?}

% My primary research interest is solving complex problems using the simplest
% AI \& fewest resources possible.

% I believe doing so will allow us to scale these algorithms to domains where less high dimensional data is available, while
% simultaneously reducing our dependency on common resources, potentially allowing
% for dense, complex networks to operate in tandem.

% Much academic effort today seems directed towards the development of AI
% with the assumption of optimal computing environments, volumes of clean data, or
% hybrid future computing paradigms like quantum variational algorithms. While
% there can no doubt such technologies provide an invaluable opportunity to
% cheat Moore's Law or access some NP hard problems, it seems increasingly
% difficult to see how anyone but a company like Google could access this kind of AI (which is
% partly what motivated me to apply.)

% Over the past 5 years, I have built a variety of Data Science solutions,
% pipelines, and proof-of-concepts for companies like Pfizer,
% TD, Honda, \& Apple to generally tackle the problems in the following domains:
% \begin{itemize}
% \item Predictive Consumer Behavior (Clickstreams, Analytics, Conversion, \ldots{})
% \item Quantitiative Financial Investing (Reinforcement, Multi-agent, Bayesian, \ldots{})
% \item Strategic Consulting (Behavioural Clustering, KPI Regression, Simulation, \ldots{})
% \end{itemize}

% In that time, one thing I have found consistently common across these multiple
% verticals is confusion. Whereas tools like Keras and Tensorflow made it possible to trade modular components to intelligent systems
% and accelerate the field academically; most private sectors, despite increasing optimism, discovered painfully that having high quality AI depended more on
% having a high quality data environment, with 80\% or more of the effort needed to
% be spent sanitizing information, building tools, and testing models, before one
% could even begin to do something intelligent with it. There is difference between the people who
% can afford to research AI, and the people who need to apply it, and this gap
% must be bridged if this technology is to get where it needs to.

% In any case, I observed many firms resolutely begin to start adapting, but
% perhaps too late, as Google entered us into the AlphaGo and Deepmind era of
% Machine Learning; where it was discovered that using bespoke digital input
% system coupled with the ability to artificially self-play with multiple agents
% across multiple GPU clusters now yielded AI that was better
% at playing some of the most advanced human games ever designed. The
% novelty of winning at Go demonstrated these techniques ability to deal with
% enormous probabilities in a rigorous manner, but tools like AlphaStar
% began to genuinely call into question what it even means to be an 'expert'
% anymore.

% I do not know what the future holds, but now that this class of AI has been
% discovered, it seems prudent to continue to iterate and optimize until we
% discover the simplest variant of it. While we will always need its robust variants, what
% I have discovered in the field is that the primary KPI for most technologies is not novelty,
% but rather ROI.

% There can be no doubt that this new class of AI has achieved a new level of
% excellence for it's given task and environment; however, now that it has been discovered it, what of the myriad of ways to reduce that denominator? Ways to apply
% Monte Carlo Tree Search to sparse data, or scale self-play via minimal
% dependencies? As a purveryor of a few of the Google ML APIs, I have found many
% to be excellent alternatives to local training in many domains; perhaps one day
% soon one might even call a variant of an AlphaZero API against some arbitrary game.

% Such work may not be glamorous, but I believe it is that practicality that may
% be pivotal in helping transition AIs from narrow to general. By reducing the cost of
% these algorithms by even a few orders of magnitude, I believe we may discover a tool that
% may level the playing field in the face of inequity, disease, and climate change.

% \pagebreak
% \lettersection{How would participating in the AI Residency help you to explore your research interests and achieve your goals?}

% I began programming to better understand what people meant by the
% term `AI`.

% When I first picked up R the summer after graduating, I held the assumption that
% only people with a Computer Science background could unravel the mysteries of
% algorithms \& data structures, and that I will always be too late to
% participate. And yet, I was constantly fascinated with it anyway; the notion
% that silicon and electricity could compete with human intuition, and everything I needed to get
% started was the computer I already had and internet connection; so I decided to get my hands dirty.

% After moving to Toronto and crashing on my friends couch, I began learning
% Python and landed my first Data Analyst job. 3 years and many more languages later I was a Data
% Scientist with experience building solutions in multiple verticals with many
% interesting people, and had continued to push my education with
% simultaneous courses at Udacity, Amazon certificates, and GCP seminars.

% Over time, I was beginning to explore the realities of Machine
% Learning, and indeed what a machine was truly capable of; only to have my
% expectations blown away every few months. Academically, it seemed there was a
% zoo of neural networks that began to do increasingly novel things, and teams
% like OpenAI and Deepmind seemed to constantly pushing the envelope of
% possibility.

% Profesionally however, I soon discovered that while being an excellent tool, much of its success was
% predicated at least partly on some human knowledge of the domain, masterful
% tuning given your algorithm, and/or volumes of perfectly formatted data that
% simply were not available in the wild. So, I resolved to emulate the pristine
% data lab environments so widely touted, and find a way nurture AI in a more
% natural environment, with the limited resources at my level.

% So I started my first company, alongside two cofounders, with the objective of researching,
% writing, implementing a novel multi-agent solution to the portfolio
% selection problem. As blockchains became popular, individuals with no financial
% knowledge wanted to participate in the world's largest distributed economy, and we
% did not want to take their money because we were not a fund. Instead we wanted to meet this
% new era of distributed systems (and logarithmic volatility) with a new class of tool; an adaptive decentralized
% index fund.

% However, to scale to this many retail users we needed a cheaper solution to
% circumvent costly deep learning methods, while still providing robust enough
% statistical momenta for use in market. In many ways we succeeded, building a
% GCP solution that used Cloud Functions, Compute severs,  and Pubsub, to
% artificially generate millions of portfolios for multi-agent random selection
% until a reasonable risk/reward position was discovered. By coupling this with a simple Binance integration, we were able to provide passive investing into Cryptocurrencies, which we used to raise \$100K @ 6\% from the University of Toronto's CDL accelerator, with a plan to integrate with Facebook Libra. For many reasons that I am not at liberty to discuss however,  that is no longer
% the case,and my cofounders and I split ways; though I still occasionally rent the AI to advise some funds.

% This gave me time to reflect, and explore many opportunities, like this residency. It has occured to
% me that not much has truly changed since I started programming. I still am
% obsessed with this new era of Machine Intelligence, and am still consistently
% impressed by the power of silicon and electricity. One could still argue that
% I'm not nearly qualified to pursue these interests, but now I would argue back
% that still doesn't stop me from needing to, since this was what I love doing.

% If given the opportunity, I would like to put this passion towards assisting
% Google in whatever capacity they feel necessary to advance this field; but if not given the opportunity I will
% continue stumbling on the shoulders of giants in whatever way I can to help aid
% in the discovery of this, the penultimate tool. As far as I'm concerned, a life spent
% pursuing AI will be a life well spent.

% \pagebreak
% \lettersection{An example of an open-ended research question or project you’ve worked on. What made it challenging and how did you overcome those challenges?}

% \subsubsection{Summary}

% Hydra AI is a proprietary machine learning model that attempts to use genetic
% algorithms and statistical price momenta for portfolio selection. It took over 2
% years to research into a whitepaper, after which work began encoding it into a
% parameterized, parallelized Python3 solution.

% Whereas current deep learning techniques attempt to classify or regress
% individual assets, requiring significant computational resources to scale and
% human intervention to assist in portfolio construction, Hydra attempts to
% generate a simultaneous portfolio of assets from a random vector efficiently,
% given the least information and resources possible using multiple agents,
% natural selection, and a financial strategy strongly competitive with the
% Limited Asset Markowitz (LAM) model.

% \subsubsection{Problem}

% The problem of selecting the optimal portfolio for a given period may be as old as finance itself, but the problem can be mathematically formalized as the \textbf{Offline Cardinality Constrained Multi Period Selection Problem}. We first describe the base case of investing over \(T\) investment periods, yielding the \textbf{Multi Period Portfolio Problem}. Let us assume that a market (the universe of all available assets) has \(n\) assets with the prices \(p = (p_1 , p_2 , . . . , p_n)\), and let \(w_{i} \geq 0\) be the proportion of capital invested in asset \(i\), as formalized in Equation \ref{eq:org2ff1f05}. To represent a portfolio, the weights (\(w_{1} , w_{2} , . . . , w_{n}\)) must therefore attempt to maximize a return \textbf{R}:

% \begin{equation}
% \label{eq:org2ff1f05}
% \begin{aligned}
% & \underset{R}{\mathrm{maximize}}
% & & R = \sum_{i=1}^{n} p_i w_i \\
% & \mathrm{subject to} & &  \sum_{i-1}^{n} w_i =1 \\
% & & & w_i \geq 0 \; \forall i = 1,...,n \\
% & & & p = (p_1 , p_2 , . . . , p_n) \\
% & & & w = (w_1 , w_2 , . . . , w_n) \\
% \end{aligned}
% \end{equation}

% To further simplify the problem, we have also added some logical constraints;
% all portfolio weights must add up to \(1\) (\(\sum_{i-1}^{n}w_{i}=1\)) and the
% investor may only have positive weight positions (no shorting).

% There have been many solutions to this problem over the decades, but perhaps the most notable is Markowitz's Nobel prize winning proposal:

% \begin{equation*}
% \begin{aligned}
% & \underset{\mupsigma_{i,j}}{\mathrm{minimize}}
% & & R = \sum_{i=1}^{n} \sum_{i=1}^{n} w_{i} \mupsigma_{i,j} w_{j} \\
% & \mathrm{subject to}
% & & \sum_{i=1}^{n} w_{i}\mupmu_{i} \geq d \\
% &&& \sum_{i=1}^{n} w_{i}  = 1 \\
% &&& w_{i} \geq 0 \: \forall i = 1,...,n \\
% \end{aligned}
% \end{equation*}

% Where \(\mupsigma_{i,j}\) is the covariance between returns of assets \(i\) and \(j\),
% \(\mupmu_{i}\) is the expected return of asset \(i\), and \(d\) is the desired level of
% expected return.

% If we add a few more constraints for realism, via Modern Portfolio Theory:
% \begin{itemize}
% \item \textbf{Offline Multiperiod Selection}: Historical data of any asset \(k\) until the
% present day \(t\) (\(p_1 , . . . , p_t\)) is available, but future price data
% (\(p_{t+1}, . . ., p_{t+n}\)) is unknown.
% \item \textbf{Cardinality Constraint}: Limit the number of assets to be held in an efficient portfolio.
% \item \textbf{Quantity Constraint}: Prescribes lower and upper bounds on the fraction of the capital invested in each asset.
% \end{itemize}

% We gain the \textbf{\textbf{Limited Asset Markowitz}} (LAM) model, the current
% financial gold standard for asset pricing via risk.

% \begin{equation*}
% \begin{aligned}
% & \underset{\mupsigma_{i,j}}{\text{minimize}}
% & & R = \sum_{i=1}^{n} \sum_{i=1}^{n} w_{i} \mupsigma_{i,j} w_{j} \\
% & \mathrm{subject to}
% & & \sum_{i=1}^{n} w_{i}\mupmu_{i} \geq d \\
% &&& \mupalpha\mupdelta_{i} \leq w_{i} \leq \mupdelta_{i} \: \forall i = 1,...,n \\
% &&& \sum_{i=1}^{n} \mupdelta_{i}  = k \\
% &&& \sum_{i=1}^{n} w_{i}  = 1 \\
% &&& w_{i} \geq 0 \: \forall i = 1,...,n \\
% &&& \mupdelta_{i}   \: \text{binary} \: \forall i = 1,...,n \\
% \end{aligned}
% \end{equation*}

% Where \(\mupsigma_{i,j}\) is still covariance between returns of assets \(i\) and \(j\),
% \(\mupmu_{i}\) is the expected return of aseet \(i\), \(\mupalpha\) and \(\mupbeta\) represent
% the lower and upper thresholds, respectively, of stock weight, \(d\) is the
% desired level of expected return, and \(k\) is the number of assets held in the
% portfolio. This allows a user to tune a portfolio to their risk and return preferences,
% using only stock returns and volatility.

% While amazing for its ability to reduce
% a massive stochastic problem two variables, the computational complexity for the
% solution of the LAM model is much greater than the one required by the classical
% Markowitz model; a convex quadratic programming problem that has a polynomial
% worst-case complexity bound, whereas the LAM model is usually modeled by adding
% binary variables, thus becoming a mixed integer quadratic programming (MIQP)
% problem, a class of considerably more difficult NP-hard problems. Other
% critiques in the decades since its inception include the fact that one should
% be able to perfect simulate any financial market if assets were indeed normally
% distributed, given these two variables, as pointed out by Mandelbrot et al. in
% the defense of Fractal Market Hypothesis over Effecient Market Hypothesis.

% In the context of Hydra AI, we use it the LAM as the primary competitor for
% performance and algorithmic complexity.

% \subsubsection{Philosophy}

% A Hydra is recursively composed of two parts: A meta AI (Hydra) that selects
% from random genetic evolutions of a candidate AI (Head), currently tuned to
% perform some single dimensional / probabilistic prediction. Here, the Head we
% have implemented is a financial investment AI, building upon a multi-agent,
% cooperative solution tailored to the offline cardinality constrained portfolio
% selection problem, generally solving it by transforming a chaotic position using
% statistical momenta descriptors and genetics to transform it into an informed
% one over time. An example of this agent updating is given in the \mathrm(updateHeadWeights()) function:

% \begin{lstlisting}[language=Python]
% @jit(forceobj=True)
% def updateHeadWeights(head, food):
%     """
%     Updates the learning rates in hydra[head]["learning_rate"]

%     Using the Chi Squared updating rule, update the weight of an asset
%     proportionally to the delta / weighted delta,
%     scaled by the learning rate.

%     If the delta is significantly larger than the weighted delta, the asset
%     has overperformed, and must be rebalanced to be given a larger weight. If not,
%     it is has underperformed, and returns a lower weight. The learning rate
%     is generated from a risk averse investor, making it price elastic.

%     >>> np.random.seed(1234)
%     >>> head = {}
%     >>> head['portfolio_weights'] = generatePortfolioWeights(10)
%     >>> head['learning_rate']     = generateLearningRate()
%     >>> head['price_relative']    = np.array([2,1,1,1,1,1,1,1,1,1])          # Black Swan Market [2, 1, ... , 1]   => Buy 0
%     >>> updatePortfolioWeights(head)
%     array([0.10629762, 0.09678675, 0.09825394, 0.10337659, 0.100581  ,
%            0.09356909, 0.1035962 , 0.09424496, 0.10183243, 0.10146142])
%     >>> head['price_relative'] = np.ones(10)/2                               # Bear Market [0.5, ...]              => No change
%     >>> updatePortfolioWeights(head)
%     array([0.10629764, 0.09678676, 0.09825395, 0.10337659, 0.10058101,
%            0.09356909, 0.1035962 , 0.09424495, 0.10183242, 0.1014614 ])
%     >>> head['price_relative'] = np.ones(10)*2                               # Bull Market [2, ...]                => No change
%     """

%     # Define variables
%     local_assets = head["portfolio_names"]
%     local_weights = head["portfolio_weights"]
%     learning_rate = head["learning_rate"]
%     price_relatives = [
%         food[asset]["price_relative"] if asset in food else 1 for asset in local_assets
%     ]

%     # Calculate New Weights
%     new_weights = [
%         local_weight
%         * (
%             learning_rate * (price_relative / np.sum(local_weights * price_relatives))
%             - 1
%         )
%         + 1
%         for local_weight, price_relative in zip(local_weights, price_relatives)
%     ]

%     # Update Head
%     head_weights = normalizeWeights(new_weights)

%     # Assign
%     head["portfolio_weights"] = head_weights

%     return head
% \end{lstlisting}

% By generating thousands of these Heads pseudorandomly over thousands of
% lifetimes, each with randomized internal learning parameters, a conditionally
% random slice of historical price data and and other tunable global
% hyper-parameters, we create a population of statistical distributions from which
% we select performant candidates from random chaos past an upper bound of
% confidence until a minimum set of optimal Heads are discovered.

% While a variety of features of traditional deep learning were emulated, the
% primary departure from traditional deep reinforcement schemes was using
% evolutionary strategies, like chromosomes and crossover, instead of error
% backpropagation. This created a feed forward style network effect that force the
% best, and strangest, agents to propogate:


% \begin{lstlisting}[language=Python]
% def seedHead(best_hydra, odd_hydra, genomes_required=10):
%     """
%     Randomly samples a genome from the best and the oddest.
%     """

%     # Extract Hydra
%     best_hydra = [[None]] + [hydra["hydra"] for hydra in best_hydra]
%     odd_hydra = [[None]] + [hydra["hydra"] for hydra in odd_hydra]
%     new_hydra = [[None]]

%     # Parse Genome
%     best_genomes = np.random.choice(
%         [genome for hydra in best_hydra for genome in hydra],
%         3 * genomes_required,
%         replace=True,
%     )

%     odd_genomes = np.random.choice(
%         [genome for hydra in odd_hydra for genome in hydra],
%         np.random.randint(1, genomes_required),
%         replace=True,
%     )

%     new_genomes = np.random.choice(
%         [genome for hydra in new_hydra for genome in hydra],
%         np.random.randint(1, genomes_required),
%         replace=True,
%     )

%     # Spread Out Mix
%     genomes = [*best_genomes] + [*odd_genomes] + [*new_genomes]

%     # Randomly Choose and Unlist
%     genome = np.random.choice(genomes, 1)[0]

%     # Return
%     return genome

% \end{lstlisting}

% After scaling these techniques across a variety of heads and times, we then
% reduce this multi-agent solution into an average position for that point of time,
% creating the final Hydra and finding the portfolio recommended for a given date;
% theoretically any arbitrary date, but typically the end of historical prices, or
% today. We believe this process to be $O(nlog(n) \cdot n\textsuperscript{2} ⟶ O(n\textsuperscript{3})$ because we are mapping a memorized random search
% $n$ times over an $n\cdotn$ matrix of prices over time, then reducing:

% \begin{lstlisting}[language=Python]
% def mapHydra():
%     """
%     Benchmarks the Hydra against a dataset, where tries to eat it from day 0 in slices.
%     """

%     # Eat
%     data = getInput(dataset)  ## Initialize

%     # Split
%     date_time, raw_data = prepareData(data)

%     # Timer
%     exec_time = time.time()

%     ## Arguments
%     splits, map_args, map_start_row, start_date, map_end_row, end_date = mapArguments(
%         raw_data, date_time
%     )

%     # Logo
%     screen_height, screen_width = logoHydra()

%     ## Map
%     results = pm.starmap(
%         metaHydra,
%         map_args,
%         raw_data,
%         exec_time=exec_time,
%         required_best=best,
%         asset_count=asset_count,
%         z_score=z_score,
%         pm_processes=processes,
%         pm_parallel=parallel,
%         pm_chunksize=1,
%         pm_pbar=not verbose,
%     )

%     # Parse
%     portfolios, favorites, sample_hydra, z_scores = (
%         [r[0] for r in results],
%         [r[1] for r in results],
%         [r[2] for r in results],
%         [r[3] for r in results],
%     )

%     # Reduce
%     meta_portfolio, total_hydra, mean_z_score = reduceResults(
%         portfolios, favorites, sample_hydra, z_scores
%     )

%     ## Save Complete
%     saved = saveOutput(
%             meta_portfolio,
%             portfolios,
%             splits,
%             start_date,
%             end_date,
%             asset_count,
%             map_end_row,
%             total_hydra,
%             mean_z_score,
%             exec_time,
%             exec_time,
%             complete=True,
%             source=save,
%     )

%     # Log
%     print("\n[@lake/hydra] map result >> \n")
%     pprint(
%         list(zip([map_arg[4] for map_arg in map_args], portfolios))
%         if mode == "benchmark"
%         else (meta_portfolio if mode == "predict" else None)
%     )

%     # Return
%     return meta_portfolio, portfolios, favorites
% \end{lstlisting}


% \subsubsection{Implementation}

% Therefore, each Head acts as a virtual investment advisor who is never absent,
% greedy, emotional, or otherwise irrational, but rather follows a simple
% quantitative strategy with randomized information consistently across time and
% price space. The results of each of these investors is tracked, and as better patterns and parameters are discovered they propagate in place of previous worse Heads via genomes, regenerating and feeding constantly on the same set of information randomly in a Darwinian process until a viable subset of candidates survive the induced chaos, and are deemed statistically superior to a lifetime of their competitors. This naturally selected mean strategy make up the Heads of the final Hydra; a probabilistic vector summarizing a high alpha \& risk adjusted position, weighted by asset preference. Here, it stops regenerating, and returns to the user, given risk tolerance and price data, the discovered assets that represent the highest trending return per unit risked for that time.

% Formally, Hydra AI includes the following features as well to help assist in
% it's ability to continue to discover alpha:
% \begin{enumerate}
% \item \textbf{\textbf{Modern}}: Python 3.7 with 3 dependencies
% \item \textbf{\textbf{Functional}}: Lazily loadable, impure functions with explicit side effects, typing, and NumPy optimizations for Cython performance where available \item \textbf{\textbf{Parallelized}}: Thread-pooled *map implementation with unary flags for multithreaded, parallel map reduce
% \item \textbf{\textbf{Parametrized}}: Tunabl*e, tractable Hyperparameters including asset count, risk tolerance \& sample size
% \item \textbf{\textbf{Async}}: Daemon logic for naive server-client setups on cloud / local machines; coordinating via PubSub and cron triggers for automated prediction across geographies or server providers
% \item \textbf{\textbf{Adversarial}}: Core strategy chosen for an academic history of strong competitiveness with standard industry techniques (Mean-Variance models), before modifications to generate 10x-100x more samples than any reference variant
% \item \textbf{\textbf{Cheap}}: Ingests only historical price time series, arbitrarily analyzing properties therein given little tuning
% \item \textbf{\textbf{Fast}}: CPU scalable by updating probabilistic single-dimension Dirichlet vectors via arithmetic instead of transforming multi-dimensional tensors via Hessian matrices (and GPUs)
% \item \textbf{\textbf{Minimal}}: Additional memoized data structures via nested dictionaries \& global dataframes to reuse memory where possible
% \item \textbf{\textbf{Dropout}}: Information deletion mechanics to prevent overfitting while encouraging chaos
% \item \textbf{\textbf{Natural Selection}}: Population sampling alongside crossover to select upper bound genomes only
% \item \textbf{\textbf{Random Search}}: Randomized lower level parameters to explore price space in a nonlinear way
% \item \textbf{\textbf{Novelty Search}}: Genetic algorithms that propagate parts of both the best and most diverse strategies
% \end{enumerate}

% Arguably, this approach still does not search all of near infinite price space, especially
% given Fractal Market Theory, nor always guarantee a safe result that should be
% immediately actioned upon. However, by pairing random and novelty search with
% stochastic price momenta and other ancillary systems, we are able to navigate
% this level of controlled chaos to a given level of risk and reward. This is especially
% given the additional volatility endemic to live assets, cryptographic or
% otherwise. By regenerating at this rate, we hypothesize that Hydra can accrue enough
% information over enough random lifetimes to notice price inertia and market
% information at a much finer detail than other / human approaches, the final
% map-reduced mean of which is generally a safe statistical bet.

% By incorporating a better understanding of chaos into Hydra, we aim not to
% replace the human investor, but rather augment him with a virtual partner whose
% objective understanding of their mutual stochastic domain can help give his
% human partner an unfair advantage that can compound over time.

\end{cvletter}


%-------------------------------------------------------------------------------
% Print the signature and enclosures with above letter informations
% \makeletterclosing

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
